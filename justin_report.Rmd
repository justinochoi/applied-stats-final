---
title: "NFL Win Projections: Bigger and Better"
subtitle: "December 4th, 2025"
author: "Justin Choi, Tengis Kelley, Heidi Tamm" 
output: pdf_document 
---

```{r}
library(tidyverse) 
library(lme4) 

train = readRDS("training_data.rds") 
test = readRDS("test_data.rds")
```

# 1. Introduction 

In the first project of the semester, we attempted to predict NFL win totals for each team using historical indicators of strength, such as previous point differential and strength of schedule. This approach yielded reasonable results, but there was much room for improvement. Notably, we failed to incorporate the actions teams take during a typical offseason to bolster their roster. In this updated version of that project, we focus on (1) **free agent signings** and (2) **amateur draft picks**. Both methods of acquiring players can dramatically elevate the outlook of teams. Marquee free agents are potentially worth multiple wins on their own, while amateur players, particularly those selected in the first round of the NFL draft, are superstars in the making.

We first discuss how we collected necessary data. We then show that knowing which free agents and amateur players a team signs during an offseason provides more accurate predictions about their next-season win total. We conclude with a summary of our findings and avenues for future research. 

# 2. Data Collection 

# 3. Modeling 

In this section, we introduce four models that will be the basis of our analysis. 

## 3.1 Baseline Model 

We wanted to start off my establishing a **baseline** model, a starting point from which we can improve on. We'll use the model that returned the best predictions from our previous report: a simple binomial model with three terms: an intercept, the previous season's point differential, and a team random effect: 

$$
\begin{aligned}
W_i &\sim \text{Binomial}(G_i, p_i) \\
\text{logit}(p_i) &= \beta_0 + \beta_1 \cdot \text{Past PD}_i + u_{Tm[i]} \\
u_{\text{Tm}[i]} &\sim \mathcal{N}(0, \sigma^2)
\end{aligned}
$$

The model was implemented using the lme4 package in R: 

```{r}
baseline = glmer(
  cbind(W, L) ~ prior_PD + (1 | Tm), 
  data = train, family = binomial(link = "logit")
)
```

## 3.2 Adding New Information 

Next, we keep the same model structure, but incorporate the additional information that we discussed in the previous section: 

$$
\begin{aligned}
W_i &\sim \text{Binomial}(G_i, p_i) \\
\text{logit}(p_i) &= \beta_0 + \beta_1 \cdot \text{Past PD}_i + \beta_2 \cdot \text{Max APY}_i \\
&+ \beta_3 \cdot \text{pick}_i + \sum_{j=1}^{J} \beta_j \cdot \text{Position}_{ij} + u_{Tm[i]} \\
u_{\text{Tm}[i]} &\sim \mathcal{N}(0, \sigma^2) \\
\end{aligned}
$$

Again, we use the lme4 package: 

```{r}
sign_lmer = glmer(
  cbind(W, L) ~ max_apy + prior_PD + pick + category + (1 | Tm), 
  data = train, family = binomial(link = "logit")
)
```

## 3.3 Bayesian Beta-Binomial 

This time, we'll keep the covariates the same, but change the model structure. From past experience, we know that NFL regular season win totals are overdispered. Even though a Binomial and Beta-Binomial distribution will give you very similar means, it might be a good idea to use the latter to better account for the variance in our distribution - as we'll see later, using the Beta-Binomial significantly sharpened the model parameters. 

$$
\begin{aligned}
W_i &\sim \text{Beta-Binomial}(G_i, p_i, \phi) \\
\text{logit}(p_i) &= \beta_0 + \beta_1 \cdot \text{Past PD}_i + \beta_2 \cdot \text{Max APY}_i \\
&+ \beta_3 \cdot \text{pick}_i + \sum_{j=1}^{J} \beta_j \cdot \text{Position}_{ij} + u_{Tm[i]} \\
u_{\text{Tm}[i]} &\sim \mathcal{N}(0, \sigma^2) \\
\phi &\sim \text{Exponential}(0.1) \\
\beta_0 &\sim \mathcal{N}(0, 0.25) \\
\beta_1,...,\beta_j &\sim \mathcal{N}(0, 4) \\
\sigma &\sim \text{Exponential}(1)
\end{aligned}
$$

This time, we'll use the brms (Bayesian Regression Modeling Software) package to implement the model. One doesn't have to be Bayesian here, but implementing a Beta-Binomial model with a random effect term is very easy using brms. We specify the priors, then write out the model formula and specifications: 

```{r, eval=FALSE}
bb_prior = bb_prior %>% 
  mutate(
    prior = case_when(
      class == 'b' ~ 'normal(0,2)', 
      class == 'Intercept' ~ 'normal(0, 0.5)', 
      class == 'sd' ~ 'exponential(1)', 
      class == 'phi' ~ 'exponential(0.1)'
    )
  )

fit_bb = brm(
  bf(
   W | trials(G) ~ prior_PD + category + pick + max_apy + (1 | Tm) 
  ), 
  data = train, family = beta_binomial, 
  prior = bb_prior, 
  cores = 4, chains = 4, 
  warmup = 1000, iter = 2500, seed = 76,
  control = list(adapt_delta = 0.95, max_treedepth = 15)
)
```

## 3.4 Gaussian Process Regression 

Lastly, we fit a Gaussian Process (GP) regression as suggested by Dr. Guinness. The idea here is that, intuitively, the team random effect shouldn't be constant from 2016 to 2023, but rather change over time. With the exception of a few dynasties (or those who "tank" for higher draft picks), teams' performances are cylical. Good teams become bad, then become good again. We believe that a GP is suitable to capture the non-linear cylical nature of team performances in the NFL. Mathematically, our model looks like this: 

$$
\begin{aligned}
W_i &\sim \text{Binomial}(G_i, p_i), \\[4pt]
\text{logit}(p_i) &= \beta_0 + g_i(\text{Year}_t) + u_{\text{Tm}[i]}, \\[6pt]
\beta_0 &\sim \mathcal{N}(0, 0.25), \\[6pt]
g_i(\cdot) &\sim \text{GP}\!\left(0, \, k(\cdot,\cdot)\right), \\[6pt]
u_{\text{Tm}[i]} &\sim \mathcal{N}(0, \sigma^2), \\[6pt]
\sigma &\sim \text{Exponential}(1)
\end{aligned}
$$

You may notice that there are no fixed-effect covariates. As we'll discuss later, adding free agent and draft information actually degraded the performance of the GP. 

Once again, we use brms to fit the model: 

```{r, eval=FALSE}
gp_prior = gp_prior %>% 
  mutate(
    prior = case_when(
      class == 'b' ~ 'normal(0,2)', 
      class == 'Intercept' ~ 'normal(0, 0.5)', 
      class == 'sd' ~ 'exponential(1)', 
      TRUE ~ prior
    )
  )

fit_gp = brm(
  bf(
    W | trials(G) ~ gp(year, by = Tm, k = 5) + (1 | Tm)
  ), 
  data = train, family = binomial, 
  prior = gp_prior, 
  cores = 4, chains = 4, 
  warmup = 1000, iter = 2500, seed = 76,
  control = list(adapt_delta = 0.95, max_treedepth = 15)
)
```

Instead of a full GP, which is computationally prohibitive, we use a Hilbert Spline approximation. By specifing k = 5, we tell brms that we want a five-dimensional basis of smooth splines to approximate the true underlying GP. The computational time and cost increases along with k; we found that k = 5 was sufficient to produce reliable results. Note that we're specifying a GP for each team, which reflects that a team like the Chiefs are less prone to year-to-year fluctuations compared to, say, the Titans. To capture the remaining variance, we also specify a random effect at the team level. 

# 4. Results

# 4.1 Model Summaries and Diagnostics 

```{r}
summary(baseline)
```

The intercept term is 0, which makes sense - on a logit scale, this corresponds to a team with a .500 winning percentage. When a team allows as many points as it gains, we should expect it to be roughly average. Past year's point differential has a significant effect on a team's winning percentage: If a team had a large point differential in the past, it's more likely to be successful in the future as well (and vice-versa). 

But how well does the model do? It's time to predict on unseen 2024 data and establish a baseline. For our error metric, we'll use both Mean Absolute Error (MAE) and Root Mean Square Error (RMSE). 

```{r}
base_prob = predict(baseline, test, type = "response") 
base_wins = 17 * base_prob 
mean(abs(test$W - base_wins)) # 2.83 
sqrt(mean((test$W - base_wins)^2)) # 3.4 
```

The results are pretty decent - on average, we're around 3 wins away from the true 2024 win totals. The RMSE is slightly higher than the MAE, in part because the 2024 NFL season seemed to feature a good number of unexpected outcomes (for example, the Washington Commanders going from a 4-13 record in 2023 to a 12-5 record in 2024). 

```{r, echo=FALSE}
summary(sign_lmer) 
```

As expected, big free agent contracts. corresponds to a greater win probability the following season. Prior point differential largely maintains its influence, while surprisingly, the draft pick number is almost completely uninformative. Also surprising is first-round running backs (categoryRB) giving their new teams a noticeable boost to their win totals, but other positions not contributing much. Before running the model, we expected the quarterback position to have a similar impact. Perhaps there are far more rookie quarterbacks who flame out than dominate the league a l√† Tom Brady or Patrick Mahomes. 

# 4.2 Predictive Capability 

# 5. Conclusion / Limitations 

